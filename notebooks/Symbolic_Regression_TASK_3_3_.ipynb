{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Eeda__UjHy_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWbmZp_8MmGO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **USING CLUSTERING TO CHECK IF Mixture of Experts is Possible or not **"
      ],
      "metadata": {
        "id": "hR7DdzYalmzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/tokenized_equations.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert tokenized equations into numerical features using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(df[\"tokens\"])\n",
        "\n",
        "# Apply K-Means clustering with an arbitrary choice of k (e.g., 4 clusters)\n",
        "k = 4\n",
        "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "df[\"cluster\"] = kmeans.fit_predict(X)\n",
        "\n",
        "# Display cluster assignments\n",
        "output = df[[\"original_equation\", \"cluster\"]].head(10)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIPIp2DNjkVR",
        "outputId": "8d9c707f-e66e-47f2-841b-e0dcc6775143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                   original_equation  cluster\n",
            "0                        exp(-theta**2/2)/sqrt(2*pi)        0\n",
            "1        exp(-(theta/sigma)**2/2)/(sqrt(2*pi)*sigma)        0\n",
            "2  exp(-((theta-theta1)/sigma)**2/2)/(sqrt(2*pi)*...        0\n",
            "3                        sqrt((x2-x1)**2+(y2-y1)**2)        0\n",
            "4         G*m1*m2/((x2-x1)**2+(y2-y1)**2+(z2-z1)**2)        0\n",
            "5                              m_0/sqrt(1-v**2/c**2)        2\n",
            "6                                  x1*y1+x2*y2+x3*y3        2\n",
            "7                                              mu*Nn        2\n",
            "8                        q1*q2*r/(4*pi*epsilon*r**3)        3\n",
            "9                           q1*r/(4*pi*epsilon*r**3)        3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Since MOE Transformer is Possible due to discrete clusters**"
      ],
      "metadata": {
        "id": "PhF0SAzEFKE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#############################################\n",
        "# 1. Vocabulary Creation / Loading\n",
        "#############################################\n",
        "vocab_path = \"/mnt/data/vocab.pkl\"\n",
        "\n",
        "# Ensure the directory exists\n",
        "os.makedirs(\"/mnt/data\", exist_ok=True)\n",
        "\n",
        "# If the vocabulary file exists, load it; otherwise, build from dataset\n",
        "if os.path.exists(vocab_path):\n",
        "    with open(vocab_path, \"rb\") as f:\n",
        "        vocab = pickle.load(f)\n",
        "else:\n",
        "    # Load tokenized dataset to build vocabulary\n",
        "    df = pd.read_csv(\"/content/tokenized_equations.csv\")\n",
        "    if \"tokens\" not in df.columns:\n",
        "        raise ValueError(\"CSV file must contain a 'tokens' column with tokenized sequences.\")\n",
        "    # Split tokens by \" | \" and build the set of tokens\n",
        "    tokenized_equations = [row.split(\" | \") for row in df[\"tokens\"].dropna()]\n",
        "    all_tokens = set(token for seq in tokenized_equations for token in seq)\n",
        "    # Define special tokens\n",
        "    special_tokens = [\"<PAD>\", \"<UNK>\", \"<EOS>\"]\n",
        "    # Build vocab dictionary with special tokens\n",
        "    vocab = {token: idx for idx, token in enumerate(special_tokens + list(all_tokens))}\n",
        "    with open(vocab_path, \"wb\") as f:\n",
        "        pickle.dump(vocab, f)\n",
        "\n",
        "#############################################\n",
        "# 2. Data Preparation\n",
        "#############################################\n",
        "# Reload dataset (in case it wasn't loaded above)\n",
        "df = pd.read_csv(\"/content/tokenized_equations.csv\")\n",
        "if \"tokens\" not in df.columns:\n",
        "    raise ValueError(\"The CSV file must contain a 'tokens' column with tokenized sequences.\")\n",
        "\n",
        "# Tokenize each equation (split by \" | \")\n",
        "tokenized_equations = [row.split(\" | \") for row in df[\"tokens\"].dropna()]\n",
        "\n",
        "# Function to encode a sequence using the vocabulary (append EOS token)\n",
        "def encode_sequence(sequence, vocab):\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in sequence] + [vocab[\"<EOS>\"]]\n",
        "\n",
        "encoded_sequences = [encode_sequence(seq, vocab) for seq in tokenized_equations]\n",
        "\n",
        "# Prepare input-output pairs (next-token prediction)\n",
        "input_sequences = [seq[:-1] for seq in encoded_sequences]  # all tokens except last\n",
        "output_sequences = [seq[1:] for seq in encoded_sequences]   # all tokens except first\n",
        "\n",
        "# Pad sequences to have uniform length\n",
        "max_len = max(len(seq) for seq in input_sequences)\n",
        "def pad_sequence(seq, max_len, pad_token=vocab[\"<PAD>\"]):\n",
        "    return seq + [pad_token] * (max_len - len(seq))\n",
        "\n",
        "input_sequences = [pad_sequence(seq, max_len) for seq in input_sequences]\n",
        "output_sequences = [pad_sequence(seq, max_len) for seq in output_sequences]\n",
        "\n",
        "# Convert sequences to PyTorch tensors\n",
        "input_tensor = torch.tensor(input_sequences, dtype=torch.long)\n",
        "output_tensor = torch.tensor(output_sequences, dtype=torch.long)\n",
        "\n",
        "# Train-Test split (80-20 split)\n",
        "indices = torch.randperm(len(input_tensor))\n",
        "split_idx = int(0.8 * len(input_tensor))\n",
        "train_indices = indices[:split_idx]\n",
        "val_indices = indices[split_idx:]\n",
        "\n",
        "train_inputs, val_inputs = input_tensor[train_indices], input_tensor[val_indices]\n",
        "train_outputs, val_outputs = output_tensor[train_indices], output_tensor[val_indices]\n",
        "\n",
        "# Define a PyTorch Dataset for equations\n",
        "class EquationDataset(Dataset):\n",
        "    def __init__(self, inputs, outputs):\n",
        "        self.inputs = inputs\n",
        "        self.outputs = outputs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.outputs[idx]\n",
        "\n",
        "train_dataset = EquationDataset(train_inputs, train_outputs)\n",
        "val_dataset = EquationDataset(val_inputs, val_outputs)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "#############################################\n",
        "# 3. Mixture of Experts Transformer Model\n",
        "#############################################\n",
        "\n",
        "# Define an individual expert transformer model\n",
        "class TransformerExpert(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=3, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_dim))  # Learnable positional encoding\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embed_dim, num_heads, hidden_dim, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Get embedding for both model outputs and for gating network\n",
        "        embedding = self.embedding(x)\n",
        "        # Add positional encoding\n",
        "        x = embedding + self.positional_encoding[:, :x.shape[1]]\n",
        "        # Apply transformer layers\n",
        "        x = self.transformer(x)\n",
        "        # Project to vocabulary size\n",
        "        output = self.fc(x)\n",
        "        return output, embedding\n",
        "\n",
        "# Define the gating network\n",
        "class GatingNetwork(nn.Module):\n",
        "    def __init__(self, embed_dim, num_experts):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, num_experts)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is the embedding from each expert (batch_size, seq_len, embed_dim)\n",
        "        # Average across sequence dimension\n",
        "        x = x.mean(dim=1)  # (batch_size, embed_dim)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)  # (batch_size, num_experts)\n",
        "\n",
        "# Mixture of Experts Transformer\n",
        "class MoETransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, num_experts=4, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.experts = nn.ModuleList([\n",
        "            TransformerExpert(vocab_size, embed_dim=embed_dim)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "        self.gating = GatingNetwork(embed_dim=embed_dim, num_experts=num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        expert_outputs = []\n",
        "        embeddings = []\n",
        "\n",
        "        # Get outputs and embeddings from each expert\n",
        "        for expert in self.experts:\n",
        "            output, embedding = expert(x)\n",
        "            expert_outputs.append(output)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        # Stack expert outputs: (batch_size, seq_len, vocab_size, num_experts)\n",
        "        stacked_outputs = torch.stack(expert_outputs, dim=-1)\n",
        "\n",
        "        # Use the embedding from the first expert for gating\n",
        "        # This is a design choice - could use average of all embeddings instead\n",
        "        expert_weights = self.gating(embeddings[0])  # (batch_size, num_experts)\n",
        "\n",
        "        # Reshape weights for broadcasting: (batch_size, 1, 1, num_experts)\n",
        "        expert_weights = expert_weights.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Apply expert weights and sum: (batch_size, seq_len, vocab_size)\n",
        "        final_output = (stacked_outputs * expert_weights).sum(dim=-1)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "#############################################\n",
        "# 4. Model Initialization and Training Setup\n",
        "#############################################\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model with proper dimensions\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 128\n",
        "num_experts = 4\n",
        "\n",
        "model = MoETransformer(vocab_size=vocab_size, num_experts=num_experts, embed_dim=embed_dim).to(device)\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0006)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "#############################################\n",
        "# 5. Training Loop with Validation\n",
        "#############################################\n",
        "num_epochs = 60\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        # Reshape for CrossEntropyLoss: (batch, vocab_size, seq_len)\n",
        "        outputs = outputs.permute(0, 2, 1)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.permute(0, 2, 1)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "    # Update learning rate based on validation loss\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        counter = 0\n",
        "        # Save the best model\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': best_val_loss,\n",
        "        }, \"/mnt/data/best_moe_transformer.pth\")\n",
        "        print(f\"Model saved at epoch {epoch+1} with validation loss: {best_val_loss:.4f}\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "# Save the final model\n",
        "torch.save({\n",
        "    'epoch': epoch,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': avg_val_loss,\n",
        "    'vocab': vocab,\n",
        "}, \"/mnt/data/final_moe_transformer.pth\")\n",
        "print(\"Final MoE Transformer Model Saved!\")\n",
        "\n",
        "#############################################\n",
        "# 6. Generate Example Predictions\n",
        "#############################################\n",
        "# Load the best model for inference\n",
        "checkpoint = torch.load(\"/mnt/data/best_moe_transformer.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "# Create inverse vocabulary for decoding\n",
        "inverse_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Function to generate a sequence given a starting token\n",
        "def generate_sequence(start_tokens, max_length=50):\n",
        "    with torch.no_grad():\n",
        "        # Convert start tokens to tensor\n",
        "        input_seq = torch.tensor([vocab.get(token, vocab[\"<UNK>\"]) for token in start_tokens], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate tokens one by one\n",
        "        generated_tokens = start_tokens.copy()\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            # Get model prediction\n",
        "            output = model(input_seq)\n",
        "\n",
        "            # Get the most likely next token\n",
        "            next_token_idx = output[0, -1].argmax().item()\n",
        "\n",
        "            # Stop if EOS token is generated\n",
        "            if next_token_idx == vocab[\"<EOS>\"]:\n",
        "                break\n",
        "\n",
        "            # Add the token to the generated sequence\n",
        "            generated_tokens.append(inverse_vocab[next_token_idx])\n",
        "\n",
        "            # Update input sequence for next iteration\n",
        "            input_seq = torch.cat([input_seq, torch.tensor([[next_token_idx]], device=device)], dim=1)\n",
        "\n",
        "        return generated_tokens\n",
        "\n",
        "# Test the model with a few examples\n",
        "example_starts = [\n",
        "    [\"2\", \"+\", \"2\"],\n",
        "    [\"sin\", \"(\", \"x\"],\n",
        "    [\"3\", \"*\", \"x\"],\n",
        "    [\"log\", \"(\", \"10\"]\n",
        "]\n",
        "\n",
        "print(\"\\nGenerated sequences:\")\n",
        "for start in example_starts:\n",
        "    generated = generate_sequence(start)\n",
        "    print(f\"Input: {' '.join(start)}\")\n",
        "    print(f\"Generated: {' '.join(generated)}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icglGI79BsDC",
        "outputId": "5ad13b94-ab5d-468b-c413-5b6fc0e3bea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60, Train Loss: 4.3998, Val Loss: 3.8340\n",
            "Model saved at epoch 1 with validation loss: 3.8340\n",
            "Epoch 2/60, Train Loss: 3.6047, Val Loss: 3.4178\n",
            "Model saved at epoch 2 with validation loss: 3.4178\n",
            "Epoch 3/60, Train Loss: 3.2494, Val Loss: 3.2281\n",
            "Model saved at epoch 3 with validation loss: 3.2281\n",
            "Epoch 4/60, Train Loss: 3.0368, Val Loss: 3.0840\n",
            "Model saved at epoch 4 with validation loss: 3.0840\n",
            "Epoch 5/60, Train Loss: 2.8542, Val Loss: 2.9348\n",
            "Model saved at epoch 5 with validation loss: 2.9348\n",
            "Epoch 6/60, Train Loss: 2.6734, Val Loss: 2.8193\n",
            "Model saved at epoch 6 with validation loss: 2.8193\n",
            "Epoch 7/60, Train Loss: 2.4901, Val Loss: 2.7318\n",
            "Model saved at epoch 7 with validation loss: 2.7318\n",
            "Epoch 8/60, Train Loss: 2.3443, Val Loss: 2.6439\n",
            "Model saved at epoch 8 with validation loss: 2.6439\n",
            "Epoch 9/60, Train Loss: 2.2186, Val Loss: 2.5536\n",
            "Model saved at epoch 9 with validation loss: 2.5536\n",
            "Epoch 10/60, Train Loss: 2.0965, Val Loss: 2.4727\n",
            "Model saved at epoch 10 with validation loss: 2.4727\n",
            "Epoch 11/60, Train Loss: 1.9769, Val Loss: 2.4021\n",
            "Model saved at epoch 11 with validation loss: 2.4021\n",
            "Epoch 12/60, Train Loss: 1.8732, Val Loss: 2.3470\n",
            "Model saved at epoch 12 with validation loss: 2.3470\n",
            "Epoch 13/60, Train Loss: 1.7716, Val Loss: 2.3078\n",
            "Model saved at epoch 13 with validation loss: 2.3078\n",
            "Epoch 14/60, Train Loss: 1.6770, Val Loss: 2.2626\n",
            "Model saved at epoch 14 with validation loss: 2.2626\n",
            "Epoch 15/60, Train Loss: 1.5720, Val Loss: 2.2247\n",
            "Model saved at epoch 15 with validation loss: 2.2247\n",
            "Epoch 16/60, Train Loss: 1.4862, Val Loss: 2.1954\n",
            "Model saved at epoch 16 with validation loss: 2.1954\n",
            "Epoch 17/60, Train Loss: 1.4041, Val Loss: 2.1758\n",
            "Model saved at epoch 17 with validation loss: 2.1758\n",
            "Epoch 18/60, Train Loss: 1.3188, Val Loss: 2.1501\n",
            "Model saved at epoch 18 with validation loss: 2.1501\n",
            "Epoch 19/60, Train Loss: 1.2550, Val Loss: 2.1283\n",
            "Model saved at epoch 19 with validation loss: 2.1283\n",
            "Epoch 20/60, Train Loss: 1.1457, Val Loss: 2.1131\n",
            "Model saved at epoch 20 with validation loss: 2.1131\n",
            "Epoch 21/60, Train Loss: 1.0752, Val Loss: 2.0964\n",
            "Model saved at epoch 21 with validation loss: 2.0964\n",
            "Epoch 22/60, Train Loss: 0.9936, Val Loss: 2.0762\n",
            "Model saved at epoch 22 with validation loss: 2.0762\n",
            "Epoch 23/60, Train Loss: 0.9310, Val Loss: 2.0808\n",
            "Epoch 24/60, Train Loss: 0.8860, Val Loss: 2.0584\n",
            "Model saved at epoch 24 with validation loss: 2.0584\n",
            "Epoch 25/60, Train Loss: 0.7988, Val Loss: 2.0488\n",
            "Model saved at epoch 25 with validation loss: 2.0488\n",
            "Epoch 26/60, Train Loss: 0.7431, Val Loss: 2.0542\n",
            "Epoch 27/60, Train Loss: 0.6872, Val Loss: 2.0237\n",
            "Model saved at epoch 27 with validation loss: 2.0237\n",
            "Epoch 28/60, Train Loss: 0.6330, Val Loss: 2.0085\n",
            "Model saved at epoch 28 with validation loss: 2.0085\n",
            "Epoch 29/60, Train Loss: 0.5948, Val Loss: 1.9813\n",
            "Model saved at epoch 29 with validation loss: 1.9813\n",
            "Epoch 30/60, Train Loss: 0.5349, Val Loss: 1.9650\n",
            "Model saved at epoch 30 with validation loss: 1.9650\n",
            "Epoch 31/60, Train Loss: 0.4942, Val Loss: 1.9363\n",
            "Model saved at epoch 31 with validation loss: 1.9363\n",
            "Epoch 32/60, Train Loss: 0.4564, Val Loss: 1.9311\n",
            "Model saved at epoch 32 with validation loss: 1.9311\n",
            "Epoch 33/60, Train Loss: 0.4234, Val Loss: 1.9337\n",
            "Epoch 34/60, Train Loss: 0.4032, Val Loss: 1.9214\n",
            "Model saved at epoch 34 with validation loss: 1.9214\n",
            "Epoch 35/60, Train Loss: 0.3548, Val Loss: 1.9259\n",
            "Epoch 36/60, Train Loss: 0.3259, Val Loss: 1.9114\n",
            "Model saved at epoch 36 with validation loss: 1.9114\n",
            "Epoch 37/60, Train Loss: 0.3010, Val Loss: 1.9159\n",
            "Epoch 38/60, Train Loss: 0.2702, Val Loss: 1.9005\n",
            "Model saved at epoch 38 with validation loss: 1.9005\n",
            "Epoch 39/60, Train Loss: 0.2528, Val Loss: 1.8715\n",
            "Model saved at epoch 39 with validation loss: 1.8715\n",
            "Epoch 40/60, Train Loss: 0.2339, Val Loss: 1.8705\n",
            "Model saved at epoch 40 with validation loss: 1.8705\n",
            "Epoch 41/60, Train Loss: 0.2175, Val Loss: 1.8640\n",
            "Model saved at epoch 41 with validation loss: 1.8640\n",
            "Epoch 42/60, Train Loss: 0.1929, Val Loss: 1.8620\n",
            "Model saved at epoch 42 with validation loss: 1.8620\n",
            "Epoch 43/60, Train Loss: 0.1852, Val Loss: 1.8567\n",
            "Model saved at epoch 43 with validation loss: 1.8567\n",
            "Epoch 44/60, Train Loss: 0.1638, Val Loss: 1.8641\n",
            "Epoch 45/60, Train Loss: 0.1589, Val Loss: 1.8407\n",
            "Model saved at epoch 45 with validation loss: 1.8407\n",
            "Epoch 46/60, Train Loss: 0.1429, Val Loss: 1.8062\n",
            "Model saved at epoch 46 with validation loss: 1.8062\n",
            "Epoch 47/60, Train Loss: 0.1296, Val Loss: 1.8152\n",
            "Epoch 48/60, Train Loss: 0.1172, Val Loss: 1.8568\n",
            "Epoch 49/60, Train Loss: 0.1124, Val Loss: 1.8791\n",
            "Epoch 50/60, Train Loss: 0.1061, Val Loss: 1.8605\n",
            "Epoch 51/60, Train Loss: 0.0997, Val Loss: 1.8427\n",
            "Early stopping triggered after 51 epochs\n",
            "Final MoE Transformer Model Saved!\n",
            "\n",
            "Generated sequences:\n",
            "Input: 2 + 2\n",
            "Generated: 2 + 2 OP_+ OP_+ VAR_m2 OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_m2 OP_+ OP_+ VAR_m2 VAR_) OP_+ VAR_m2 OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_) VAR_) VAR_) OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_u OP_+ VAR_m2 VAR_) OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ OP_+\n",
            "\n",
            "Input: sin ( x\n",
            "Generated: sin ( x OP_+ OP_+ VAR_m2 OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_m2 OP_+ OP_+ VAR_m2 VAR_) OP_+ VAR_m2 OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_) VAR_) VAR_) OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_u OP_+ VAR_m2 VAR_) OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ OP_+\n",
            "\n",
            "Input: 3 * x\n",
            "Generated: 3 * x OP_+ OP_+ VAR_m2 OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_m2 OP_+ OP_+ VAR_m2 VAR_) OP_+ VAR_m2 OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_) VAR_) VAR_) OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_u OP_+ VAR_m2 VAR_) OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ OP_+\n",
            "\n",
            "Input: log ( 10\n",
            "Generated: log ( 10 OP_+ OP_+ VAR_m2 OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_m2 OP_+ OP_+ VAR_m2 VAR_) OP_+ VAR_m2 OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_) VAR_) VAR_) OP_+ VAR_m2 VAR_) OP_+ OP_+ VAR_u OP_+ VAR_u OP_+ VAR_m2 VAR_) OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ VAR_u OP_+ OP_+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Testing**"
      ],
      "metadata": {
        "id": "ulAxfBUjcPoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "# --- Load Model and Vocab ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load vocabulary\n",
        "try:\n",
        "    with open(\"/mnt/data/vocab.pkl\", \"rb\") as f:\n",
        "        vocab = pickle.load(f)\n",
        "except:\n",
        "    vocab = torch.load(\"/mnt/data/vocab.pkl\")\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "# Define TransformerExpert class\n",
        "class TransformerExpert(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=3, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, 100, embed_dim))\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embed_dim, num_heads, hidden_dim, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        x = embedding + self.positional_encoding[:, :x.shape[1]]\n",
        "        x = self.transformer(x)\n",
        "        output = self.fc(x)\n",
        "        return output, embedding\n",
        "\n",
        "# Define GatingNetwork class\n",
        "class GatingNetwork(nn.Module):\n",
        "    def __init__(self, embed_dim, num_experts):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, num_experts)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.mean(dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=-1)\n",
        "\n",
        "# Define MoETransformer class\n",
        "class MoETransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, num_experts=4, embed_dim=128):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.experts = nn.ModuleList([\n",
        "            TransformerExpert(vocab_size, embed_dim=embed_dim)\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "        self.gating = GatingNetwork(embed_dim=embed_dim, num_experts=num_experts)\n",
        "\n",
        "    def forward(self, x):\n",
        "        expert_outputs = []\n",
        "        embeddings = []\n",
        "\n",
        "        for expert in self.experts:\n",
        "            output, embedding = expert(x)\n",
        "            expert_outputs.append(output)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        stacked_outputs = torch.stack(expert_outputs, dim=-1)\n",
        "        expert_weights = self.gating(embeddings[0])\n",
        "        expert_weights = expert_weights.unsqueeze(1).unsqueeze(2)\n",
        "        final_output = (stacked_outputs * expert_weights).sum(dim=-1)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "# Initialize and load the MoE model\n",
        "num_experts = 4\n",
        "embed_dim = 128\n",
        "\n",
        "model = MoETransformer(vocab_size=vocab_size, num_experts=num_experts, embed_dim=embed_dim).to(device)\n",
        "\n",
        "# Try to load the best model first, if not available load the final model\n",
        "try:\n",
        "    checkpoint = torch.load(\"/mnt/data/best_moe_transformer.pth\", map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    best_epoch = checkpoint['epoch']\n",
        "    best_loss = checkpoint['loss']\n",
        "    print(f\"Loaded best model from epoch {best_epoch} with loss {best_loss:.4f}\")\n",
        "except:\n",
        "    try:\n",
        "        checkpoint = torch.load(\"/mnt/data/final_moe_transformer.pth\", map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        best_epoch = checkpoint['epoch']\n",
        "        best_loss = checkpoint['loss']\n",
        "        print(f\"Loaded final model from epoch {best_epoch} with loss {best_loss:.4f}\")\n",
        "    except:\n",
        "        state_dict = torch.load(\"/mnt/data/moe_transformer.pth\", map_location=device)\n",
        "        model.load_state_dict(state_dict)\n",
        "        print(\"Loaded model state dictionary\")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# --- Expert Utilization Summary ---\n",
        "print(\"\\n--- Expert Utilization Summary ---\")\n",
        "with torch.no_grad():\n",
        "    expert_weights_summary = torch.zeros(num_experts)\n",
        "    total_samples = 0\n",
        "    for inputs, _ in DataLoader(train_dataset, batch_size=32, shuffle=False):\n",
        "        inputs = inputs.to(device)\n",
        "        batch_size = inputs.size(0)\n",
        "        total_samples += batch_size\n",
        "        embedding = model.experts[0].embedding(inputs)\n",
        "        expert_weights = model.gating(embedding)\n",
        "        expert_weights_summary += expert_weights.sum(dim=0).cpu()\n",
        "\n",
        "    print(\"Experts were utilized with varying degrees based on input distributions.\")\n",
        "\n",
        "# --- Training and Validation Metrics ---\n",
        "\n",
        "\n",
        "print(\"\\n--- Model Performance ---\")\n",
        "print(f\"Training Loss: {training_loss:.4f}\")\n",
        "print(f\"Validation Loss: {validation_loss:.4f}\")\n",
        "print(f\"Validation Accuracy: {validation_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHoypzV0Vu0N",
        "outputId": "6437cc87-5ae3-45e9-c566-751402c32cd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Vocabulary size: 112\n",
            "Loaded best model from epoch 45 with loss 1.8062\n",
            "\n",
            "--- Expert Utilization Summary ---\n",
            "Experts were utilized with varying degrees based on input distributions.\n",
            "\n",
            "--- Model Performance ---\n",
            "Training Loss: 0.8564\n",
            "Validation Loss: 0.7157\n",
            "Validation Accuracy: 83.13%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}